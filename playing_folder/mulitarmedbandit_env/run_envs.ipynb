{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we run different mulitarmed bandit models from the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiarmedbandits.environments import GaussianBanditEnv, BernoulliBanditEnv, DistParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Bernoulli Bandit by playing each arm once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 0 gave a reward of: 0.0\n",
      "optimal action was 0 times played\n",
      "the new regret is 0.9\n",
      "Arm 1 gave a reward of: 1.0\n",
      "optimal action was 1 times played\n",
      "the new regret is 0.8\n",
      "Arm 2 gave a reward of: 0.0\n",
      "optimal action was 1 times played\n",
      "the new regret is 1.7000000000000002\n",
      "Arm 3 gave a reward of: 1.0\n",
      "optimal action was 1 times played\n",
      "the new regret is 1.6\n"
     ]
    }
   ],
   "source": [
    "# initialize mean parameter\n",
    "bernoulli_parameter = DistParameter(mean_parameter=[0.1, 0.9, 0.1, 0.1])\n",
    "\n",
    "MAXSTEPS = len(bernoulli_parameter.mean_parameter)\n",
    "bandit_env = BernoulliBanditEnv(\n",
    "    distr_params=bernoulli_parameter, max_steps=MAXSTEPS\n",
    ")\n",
    "for play_action in range(MAXSTEPS):\n",
    "    _, get_reward, _, _ = bandit_env.step(play_action)\n",
    "    print(\"Arm\", play_action, \"gave a reward of:\", get_reward)\n",
    "    print(f\"optimal action was {bandit_env.played_optimal} times played\")\n",
    "    print(f\"the new regret is {bandit_env.regret}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Gaussian Bandit by running each arm once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 0 gave a reward of: 1.3596263898434757\n",
      "optimal action was 0 times played\n",
      "the new regret is 3.6403736101565243\n",
      "Arm 1 gave a reward of: 2.917153519647599\n",
      "optimal action was 0 times played\n",
      "the new regret is 5.723220090508925\n",
      "Arm 2 gave a reward of: 4.028285451059308\n",
      "optimal action was 1 times played\n",
      "the new regret is 6.694934639449618\n",
      "Arm 3 gave a reward of: 0.877560511756291\n",
      "optimal action was 1 times played\n",
      "the new regret is 10.817374127693327\n"
     ]
    }
   ],
   "source": [
    "arm_means_gaussian = [1.0, 2.0, 5.0, 2.0]\n",
    "guassian_params = DistParameter(\n",
    "    mean_parameter=[1.0, 2.0, 5.0, 2.0],\n",
    "    scale_parameter=[1.0 for _ in range(len(arm_means_gaussian))],\n",
    ")\n",
    "MAXSTEPSGAUSSIAN = len(guassian_params.mean_parameter)\n",
    "bandit_env = GaussianBanditEnv(\n",
    "    distr_params=guassian_params, max_steps=MAXSTEPSGAUSSIAN\n",
    ")\n",
    "for play_action in range(MAXSTEPS):\n",
    "    _, get_reward, _, _ = bandit_env.step(play_action)\n",
    "    print(\"Arm\", play_action, \"gave a reward of:\", get_reward)\n",
    "    print(f\"optimal action was {bandit_env.played_optimal} times played\")\n",
    "    print(f\"the new regret is {bandit_env.regret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
